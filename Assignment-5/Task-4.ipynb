{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47839dff",
   "metadata": {},
   "source": [
    "Task 4: Conceptual Questions\n",
    "\n",
    "1. Entropy and Information Gain:\n",
    "Entropy measures uncertainty. Information Gain is the reduction in entropy when a feature is used to split the data — it's how much “information” the feature gives about the class.\n",
    "\n",
    "2. Gini vs Entropy:\n",
    "Both are impurity measures. Gini is faster to compute and prefers pure splits. Entropy is more theoretically sound and can be better with imbalanced datasets.\n",
    "\n",
    "3. Overfitting in Decision Trees:\n",
    "Trees overfit when they grow too deep and memorize training data. Prevent it using pruning, setting max depth, or limiting min samples per split."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
